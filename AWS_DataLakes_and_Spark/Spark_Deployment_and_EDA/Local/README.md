These notebooks highlight two different ways we can manipulate data using PySpark

* Imperative Programming
  * Use Spark's DataFrame API 
  * See Spark_EDA_Python.ipynb

* Declarative Programming
  * Use Spark's SQL API
  * See Spark_EDA_SQL.ipynb

DataFrames and Spark SQL offer similiar functionality-- choice is largely a matter of preference

Noting here that these notebooks were run locally 
