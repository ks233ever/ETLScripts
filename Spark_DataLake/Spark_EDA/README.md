There are two different ways we can manipulate data using Spark

* Imperative Programming
  *Use Spark's Python API PySpark
  *See Spark_EDA_Python.ipynb

*Declarative Programming
  *Use Spark's SQL API
  *See Spark_EDA_SQL.ipynb

PySpark and Spark SQL offer similiar functionality-- choice is largely a matter of preference

Noting here that these notebooks were run locally 
